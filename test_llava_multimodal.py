"""
Test LLaVA multimodal inference with real driving images
"""

import sys
sys.path.append('/Users/vivitsashankar/Desktop/workspace/drive-qa')

from rag.pipeline.rag_pipeline import RAGPipeline
from rag.models.model_loader import ModelLoader
from loguru import logger
import json
import os

def test_llava_with_driving_images():
    """Test LLaVA with actual driving scene images"""
    try:
        logger.info("Testing LLaVA multimodal inference on CPU...")
        
        # Initialize pipeline with CPU optimization
        pipeline = RAGPipeline(
            data_path="data/concatenated_data/concatenated_data.json",
            device="cpu",  # Force CPU
            quantization="8bit"  # Use 8-bit quantization for CPU efficiency
        )
        
        # Test multimodal queries that require both text context and images
        multimodal_queries = [
            "What objects can you see in the camera images?",
            "Is there a car in front of the ego vehicle?", 
            "What is the traffic situation ahead?",
            "Describe what you see in the front camera"
        ]
        
        scene_id = 1
        keyframe_id = 1
        
        print(f"ğŸ¥ Testing LLaVA on Scene {scene_id}, Keyframe {keyframe_id}")
        print("=" * 60)
        
        for i, query in enumerate(multimodal_queries, 1):
            print(f"\nğŸ” Query {i}: {query}")
            
            # Force multimodal strategy to test LLaVA
            result = pipeline.answer_question(
                query=query,
                scene_id=scene_id,
                keyframe_id=keyframe_id,
                force_strategy="multimodal"  # Force LLaVA usage
            )
            
            print(f"âœ… Success: {result['success']}")
            if result['success']:
                print(f"ğŸ¤– Answer: {result['answer']}")
                print(f"ğŸ¯ Strategy: {result.get('generation_metadata', {}).get('strategy', 'unknown')}")
                
                # Show context info
                generation_meta = result.get('generation_metadata', {})
                if 'context_summary' in generation_meta:
                    context = generation_meta['context_summary']
                    print(f"ğŸ“Š Context: {context.get('total_objects', 0)} objects, Speed: {context.get('ego_speed', 'N/A')} m/s")
            else:
                print(f"âŒ Error: {result['answer']}")
                # If multimodal fails, try with hybrid
                print("ğŸ”„ Retrying with hybrid strategy...")
                result = pipeline.answer_question(
                    query=query,
                    scene_id=scene_id,
                    keyframe_id=keyframe_id,
                    force_strategy="hybrid"
                )
                print(f"âœ… Hybrid Success: {result['success']}")
                print(f"ğŸ¤– Hybrid Answer: {result['answer']}")
            
            print("-" * 40)
        
        # Test dataset QA with multimodal
        print(f"\nğŸ¯ Testing Dataset QA with multimodal...")
        dataset_result = pipeline.answer_dataset_qa(
            scene_id=scene_id,
            keyframe_id=keyframe_id,
            qa_type="perception",
            qa_serial=1,
            force_strategy="multimodal"
        )
        
        if dataset_result['success']:
            print(f"ğŸ“‹ Dataset Question: {dataset_result['query']}")
            print(f"ğŸ¤– Generated Answer: {dataset_result['answer']}")
            print(f"âœ… Ground Truth: {dataset_result['dataset_info']['ground_truth']}")
            print(f"ğŸ“Š Overall Score: {dataset_result['dataset_info']['evaluation']['overall_score']:.3f}")
        else:
            print(f"âŒ Dataset QA failed: {dataset_result['answer']}")
        
        # Show memory usage
        memory_stats = pipeline.model_loader.get_memory_usage()
        print(f"\nğŸ’¾ Memory Usage:")
        print(f"   LLaVA Loaded: {memory_stats['llava_loaded']}")
        print(f"   Llama Loaded: {memory_stats['llama_loaded']}")
        
        pipeline.unload_models()
        return True
        
    except Exception as e:
        logger.error(f"LLaVA multimodal test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """Run LLaVA multimodal test"""
    logger.info("Starting LLaVA multimodal test...")
    
    success = test_llava_with_driving_images()
    
    if success:
        print("\nğŸ‰ LLaVA multimodal test completed!")
        print("ğŸ”¥ Vision-Language capabilities working!")
        print("ğŸ“± CPU-optimized inference successful!")
    else:
        print("\nâŒ LLaVA multimodal test failed")

if __name__ == "__main__":
    main()